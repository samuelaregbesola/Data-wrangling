{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "This project, involved the challenge of integrating data from three disparate sources into a single, usable dataset. The final dataset was used to produce meaningful insights of the WeRateDogs Tweets  <br><br>\n",
    "<figure>\n",
    "    <img src=\"./dogtionary-combined.png\" alt=\"dogtionary\"/><br>\n",
    "    <center><figcaption><b>Fig. 1.</b> Dogtionary</figcaption></center>\n",
    "</figure>\n",
    "<br><br>\n",
    "\n",
    "## Data gathering\n",
    "\n",
    "The primary source of information, <b>twitter_archive</b>, was obtained through manual download and presented a multitude of quality and tidiness issues upon initial inspection. Two additional datasets, <b>image_prediction</b> and <b>tweet_jsons</b>, were collected through online methods, with image_prediction utilizing the \"requests\" library and image_prediction being gathered through the Twitter API (although the provided data was used, as the author's developer account had not yet been approved). While both of these datasets presented fewer problems than twitter_archive, they still required some level of cleaning and refinement.\n",
    "\n",
    "## Data Assessing\n",
    "\n",
    "Problems with the datasets were identified visually and programmatically. Consistency, validity, accuracy and consistency issues were assessed. The problems were grouped into Quality and Tidiness issues. Ten quality issues, and two tidiness issues were identified.\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "\n",
    "This is perhaps the most challenging part of the wrangling process. Define, code, test was the approach employed to fix the identified issues in the assessment section. The twitter_archive dataframe was quite messy. The first task was to remove records that were irrelevant to the analysis, such as retweets and replies, identified through the \"in_reply_to_status_id\" and \"retweeted_status_id\" columns. After this initial culling, missing were inputed by using the twitter profile link, and tweet id. Columns with wrong datatypes were addressed, and inconsistent naming conventions were also treated. \n",
    "\n",
    "\n",
    "Additionally,  four separate columns designated for dog stage were consolidated into a single column. Other minor issues were resolved, as detailed in the code file \"wrangle_act.ipynb.\"\n",
    "\n",
    "The image_prediction dataset was comparatively straightforward to clean. For image_prediction, only the prediction with the highest confidence was used (as long as it pertained to a dog breed), with all other columns being discarded.\n",
    "\n",
    "Finally, The \"retweet_count\" and \"favorite_count\" columns from tweet_json were extracted and merged with enhanced_archive via the \"tweet_id\" column. This was in turn merged with the image_prediction dataset to create the final dataset. \n",
    "\n",
    "## Conclusion\n",
    "While the data was initially far from ideal, a significant amount of effort resulted in a well-formed, usable dataset. Of course, there is always room for further improvement, but the end result of the data wrangling process was a marked improvement over the original sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
